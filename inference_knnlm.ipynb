{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "576000b4-edab-4e9e-a93a-36dac379ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_nmf import NMF, NMFConfig\n",
    "from data_utils import get_reddit_data\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b6aa876-5f20-490d-8464-87119b7831f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def build_datastore_embedding(sentences, model, tokenizer, device):\n",
    "    print('building datastore embeddings')\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize an empty list to hold the sentence embeddings\n",
    "    all_sentence_embeddings = []\n",
    "    \n",
    "    # Process sentences in batches\n",
    "    batch_size = 64\n",
    "    \n",
    "    for i in tqdm(range(0, len(sentences), batch_size), total=len(sentences)//batch_size):\n",
    "        batch_sentences = sentences[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize sentences\n",
    "        encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        \n",
    "        # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input, output_hidden_states=True)\n",
    "    \n",
    "        # # Perform pooling\n",
    "        # sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask']).cpu()\n",
    "    \n",
    "        # insteading of doing pooling, can just use last hidden state\n",
    "        sentence_embeddings = model_output.hidden_states[-1][:,0,:]\n",
    "        \n",
    "        all_sentence_embeddings.append(sentence_embeddings.cpu())\n",
    "    \n",
    "    # Concatenate all batched embeddings\n",
    "    all_sentence_embeddings = torch.cat(all_sentence_embeddings, dim=0)\n",
    "    \n",
    "    all_sentence_embeddings = F.normalize(all_sentence_embeddings, p=2, dim=1).cpu()\n",
    "    post_embeddings = all_sentence_embeddings\n",
    "\n",
    "    return post_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "486ef419-c993-4c25-93d6-288dd739d5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNLMForTuningHyperParams:\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        data_path,\n",
    "        tokenizer_name_or_path = 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "        model_name_or_path = 'saved_models'\n",
    "    ):\n",
    "\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        sentence_embedding_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "        nmf = NMF.from_pretrained(model_name_or_path).to(device)\n",
    "        nmf.eval()\n",
    "        sentence_embedding_model = nmf.model\n",
    "        \n",
    "        training_dataset, validation_dataset, movie_vocab = get_reddit_data_with_heldout(data_path, heldout_portion=0.2)\n",
    "        movie_vocab = [m.split(' (')[0] for m in movie_vocab] # remove the year at end\n",
    "        training_posts = sorted(list(set(training_dataset['context'])))\n",
    "        trainingpost2idx = dict(zip(\n",
    "            training_posts, \n",
    "            list(range(len(training_posts)))\n",
    "        ))\n",
    "\n",
    "        trainingpostidx2movies = defaultdict(list)\n",
    "        movie_from_posts = []\n",
    "        for i in range(len(training_dataset['context'])):\n",
    "            post_idx = trainingpost2idx[training_dataset['context'][i]]\n",
    "            # the split is for removing year at end, e.g. \" (2019)\"\n",
    "            # movie = movie_vocab[training_dataset['label'][i]].split(' (')[0] \n",
    "            trainingpostidx2movies[post_idx].append(training_dataset['label'][i])\n",
    "\n",
    "        post_embeddings = build_datastore_embedding(\n",
    "            sentences = training_posts, \n",
    "            model = sentence_embedding_model, \n",
    "            tokenizer = sentence_embedding_tokenizer, \n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        ).to(device)\n",
    "\n",
    "        self.device = device\n",
    "        self.sentence_embedding_tokenizer = sentence_embedding_tokenizer\n",
    "        self.sentence_embedding_model = sentence_embedding_model\n",
    "        self.movie_vocab = movie_vocab\n",
    "        self.training_posts = training_posts\n",
    "        self.post_embeddings = post_embeddings\n",
    "        self.trainingpostidx2movies = trainingpostidx2movies\n",
    "        self.nmf = nmf\n",
    "        self.validation_dataset = validation_dataset\n",
    "\n",
    "    def encode_sentences(self, batch_sentences):\n",
    "        with torch.no_grad():\n",
    "            encoded_input = self.sentence_embedding_tokenizer(\n",
    "                batch_sentences, \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                return_tensors='pt'\n",
    "            ).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                model_output = self.sentence_embedding_model(**encoded_input, output_hidden_states=True)\n",
    "            sentence_embeddings = model_output.hidden_states[-1][:,0,:]\n",
    "    \n",
    "        return sentence_embeddings\n",
    "\n",
    "    def top_post_ids_retrieval(self, query):\n",
    "    \n",
    "        query_embedding = self.encode_sentences(query)[0].reshape(1, -1)\n",
    "        cosine_similarities = torch.cosine_similarity(query_embedding, self.post_embeddings).cpu().numpy()\n",
    "        sorted_indices = np.argsort(cosine_similarities)[::-1]\n",
    "        return sorted_indices, cosine_similarities[sorted_indices]\n",
    "\n",
    "    def count_based_probability(\n",
    "        self,\n",
    "        query, \n",
    "        num_posts_to_consider=30, \n",
    "        return_logits=False, \n",
    "        distance_weighting=False,\n",
    "        temperature = 1.0\n",
    "    ):\n",
    "        relevant_post_ids, similarities = self.top_post_ids_retrieval(query)\n",
    "        movie_pool = defaultdict(int)\n",
    "        probas = np.zeros(len(self.movie_vocab))\n",
    "        for i, id in enumerate(relevant_post_ids[:num_posts_to_consider]):\n",
    "            for movieid in self.trainingpostidx2movies[id]:\n",
    "                if distance_weighting:\n",
    "                    probas[movieid] += similarities[i]/temperature\n",
    "                else:\n",
    "                    probas[movieid] += 1\n",
    "        if return_logits:\n",
    "            return probas\n",
    "        probas = torch.nn.functional.softmax(torch.from_numpy(probas), dim=-1)\n",
    "        return probas\n",
    "\n",
    "    def predictor_probability(\n",
    "        self, \n",
    "        query,\n",
    "        return_logits = False\n",
    "    ):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model_input = self.sentence_embedding_tokenizer(\n",
    "                query, \n",
    "                return_tensors='pt', \n",
    "                max_length=368, \n",
    "                truncation=True\n",
    "            )\n",
    "\n",
    "            logits = reddit_knnlm_recommender.nmf(\n",
    "                            model_input['input_ids'].to(self.device), \n",
    "                            model_input['token_type_ids'].to(self.device), \n",
    "                            model_input['attention_mask'].to(self.device),\n",
    "                            labels=None\n",
    "                        ).logits\n",
    "        if return_logits:\n",
    "            return logits.cpu().numpy()\n",
    "        probas = F.softmax(logits, dim=-1)[0]\n",
    "        return  probas.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1bbc69f-cf44-4e29-b8a4-9a9fc21bfc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNLMRecommender:\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer_name_or_path = 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "        model_name_or_path = 'saved_models',\n",
    "        data_path = 'reddit/reddit_large_train.csv'\n",
    "    ):\n",
    "\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        sentence_embedding_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "        nmf = NMF.from_pretrained(model_name_or_path).to(device)\n",
    "        nmf.eval()\n",
    "        sentence_embedding_model = nmf.model\n",
    "        \n",
    "        training_dataset, movie_vocab = get_reddit_data(data_path)\n",
    "        movie_vocab = [m.split(' (')[0] for m in movie_vocab] # remove the year at end\n",
    "        training_posts = sorted(list(set(training_dataset['context'])))\n",
    "        trainingpost2idx = dict(zip(\n",
    "            training_posts, \n",
    "            list(range(len(training_posts)))\n",
    "        ))\n",
    "\n",
    "        trainingpostidx2movies = defaultdict(list)\n",
    "        movie_from_posts = []\n",
    "        for i in range(len(training_dataset['context'])):\n",
    "            post_idx = trainingpost2idx[training_dataset['context'][i]]\n",
    "            # the split is for removing year at end, e.g. \" (2019)\"\n",
    "            # movie = movie_vocab[training_dataset['label'][i]].split(' (')[0] \n",
    "            trainingpostidx2movies[post_idx].append(training_dataset['label'][i])\n",
    "\n",
    "        post_embeddings = build_datastore_embedding(\n",
    "            sentences = training_posts, \n",
    "            model = sentence_embedding_model, \n",
    "            tokenizer = sentence_embedding_tokenizer, \n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "\n",
    "        self.device = device\n",
    "        self.sentence_embedding_tokenizer = sentence_embedding_tokenizer\n",
    "        self.sentence_embedding_model = sentence_embedding_model\n",
    "        self.movie_vocab = movie_vocab\n",
    "        self.training_posts = training_posts\n",
    "        self.post_embeddings = post_embeddings\n",
    "        self.trainingpostidx2movies = trainingpostidx2movies\n",
    "        self.nmf = nmf\n",
    "\n",
    "    def encode_sentences(self, batch_sentences):\n",
    "        with torch.no_grad():\n",
    "            encoded_input = self.sentence_embedding_tokenizer(\n",
    "                batch_sentences, \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                return_tensors='pt'\n",
    "            ).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                model_output = self.sentence_embedding_model(**encoded_input, output_hidden_states=True)\n",
    "            sentence_embeddings = model_output.hidden_states[-1][:,0,:]\n",
    "    \n",
    "        return sentence_embeddings\n",
    "\n",
    "    def top_post_ids_retrieval(self, query):\n",
    "    \n",
    "        query_embedding = self.encode_sentences(query)[0].reshape(1, -1).cpu()\n",
    "        cosine_similarities = cosine_similarity(query_embedding, self.post_embeddings)\n",
    "        sorted_indices = np.argsort(cosine_similarities[0])[::-1]\n",
    "        return sorted_indices, cosine_similarities[0][sorted_indices]\n",
    "\n",
    "    def count_based_probability(\n",
    "        self,\n",
    "        query, \n",
    "        num_posts_to_consider=30, \n",
    "        return_logits=False, \n",
    "        distance_weighting=False\n",
    "    ):\n",
    "        relevant_post_ids, similarities = self.top_post_ids_retrieval(query)\n",
    "        movie_pool = defaultdict(int)\n",
    "        probas = np.zeros(len(self.movie_vocab))\n",
    "        for i, id in enumerate(relevant_post_ids[:num_posts_to_consider]):\n",
    "            for movieid in self.trainingpostidx2movies[id]:\n",
    "                if distance_weighting:\n",
    "                    probas[movieid] += similarities[i]\n",
    "                else:\n",
    "                    probas[movieid] += 1\n",
    "        if return_logits:\n",
    "            return probas\n",
    "        probas = torch.nn.functional.softmax(torch.from_numpy(probas), dim=-1)\n",
    "        return probas\n",
    "\n",
    "    def predictor_probability(\n",
    "        self, \n",
    "        query,\n",
    "        return_logits = False\n",
    "    ):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model_input = self.sentence_embedding_tokenizer(\n",
    "                query, \n",
    "                return_tensors='pt', \n",
    "                max_length=368, \n",
    "                truncation=True\n",
    "            )\n",
    "\n",
    "            logits = reddit_knnlm_recommender.nmf(\n",
    "                            model_input['input_ids'].to(self.device), \n",
    "                            model_input['token_type_ids'].to(self.device), \n",
    "                            model_input['attention_mask'].to(self.device),\n",
    "                            labels=None\n",
    "                        ).logits\n",
    "        if return_logits:\n",
    "            return logits.cpu().numpy()\n",
    "        probas = F.softmax(logits, dim=-1)[0]\n",
    "        return  probas.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "062a946c-083f-4853-a2df-775f3c749c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_eligible_entities(resource_path=''):\n",
    "    entity2id = eval(open(resource_path+'entity2id.json', 'r').readlines()[0])\n",
    "    id2entity = {v:k for k,v in entity2id.items()}\n",
    "    eligible_entities = [id2entity[idx].split('/')[-1].split('_(')[0].rstrip('>').replace('_',' ') for idx in \\\n",
    "    eval(open(resource_path+'item_ids.json', 'r').readlines()[0])]\n",
    "    return eligible_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e0e4ab1-eaa5-4b47-86e4-3438c36474d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspired_eligible_entities = set(get_eligible_entities('./entity_assets/inspired/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d078411-671a-4921-80e9-f9b52020a418",
   "metadata": {},
   "outputs": [],
   "source": [
    "redial_eligible_entities = set(get_eligible_entities('./entity_assets/redial/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f09b67-40cc-46f4-9e9a-e9b29e85c4e7",
   "metadata": {},
   "source": [
    "## Inspired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d0a036d-bcfe-4d28-9542-5e928b33dc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interaction preservance rate at 20000 items\n",
      "1.0\n",
      "num items  1436\n",
      "flattening posts into training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 584/584 [00:00<00:00, 930297.58it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 147/147 [00:00<00:00, 1076025.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building datastore embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:01,  8.09it/s]                                                                                        \n",
      "100%|███████████████████████████████████████████████████████████████████████| 475/475 [00:02<00:00, 186.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 475/475 [00:02<00:00, 187.14it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 475/475 [00:02<00:00, 186.30it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 475/475 [00:02<00:00, 185.17it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 475/475 [00:02<00:00, 183.85it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 475/475 [00:02<00:00, 182.72it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 475/475 [00:02<00:00, 181.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the recommended number of neighbors to use is  90\n",
      "interaction preservance rate at 20000 items\n",
      "1.0\n",
      "num items  1436\n",
      "flattening posts into training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 731/731 [00:00<00:00, 1052897.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building datastore embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:01,  7.64it/s]                                                                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Retrieval -------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 211/211 [00:01<00:00, 183.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@1 0.02843601895734597 ; se:  0.01146992169674842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 179294.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@5 0.08530805687203792 ; se:  0.019276261283512997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 195579.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@10 0.13744075829383887 ; se:  0.023759789586756046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 187340.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@20 0.20853080568720378 ; se:  0.02803447781764188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 165978.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@50 0.26540284360189575 ; se:  0.03046967065084662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 155399.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@100 0.33649289099526064 ; se:  0.03260626767859446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 108097.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@300 0.4881516587677725 ; se:  0.03449358924634331\n",
      "-------------------- Recommend -------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 213.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@1 0.0 ; se:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 183457.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@5 0.0 ; se:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 197456.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@10 0.014218009478672985 ; se:  0.008169588695462889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 189223.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@20 0.023696682464454975 ; se:  0.01049606495529992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 166478.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@50 0.07582938388625593 ; se:  0.018267755671268843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 152533.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@100 0.17061611374407584 ; se:  0.025958415047638204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 104043.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@300 0.3981042654028436 ; se:  0.033779203187119355\n",
      "-------------------- R+R (rerank) -------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 211/211 [00:01<00:00, 108.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@1 0.023696682464454975 ; se:  0.010496064955299918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 174315.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@5 0.08530805687203792 ; se:  0.019276261283512997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 197500.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@10 0.12322274881516587 ; se:  0.022681952442793474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 189344.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@20 0.1943127962085308 ; se:  0.02730387495943987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 170355.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@50 0.25118483412322273 ; se:  0.029927771242945177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 153405.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@100 0.3412322274881517 ; se:  0.032717608075019784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 108309.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@300 0.46445497630331756 ; se:  0.03441598128848769\n",
      "-------------------- R+R (rerank) with small gamma (cleaner solution for paper) -------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 211/211 [00:01<00:00, 106.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@1 0.023696682464454975 ; se:  0.010496064955299918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 176964.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@5 0.08530805687203792 ; se:  0.019276261283512997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 192600.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@10 0.12322274881516587 ; se:  0.022681952442793474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 181129.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@20 0.1943127962085308 ; se:  0.02730387495943987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 164070.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@50 0.25118483412322273 ; se:  0.029927771242945177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 152717.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@100 0.3412322274881517 ; se:  0.032717608075019784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 211/211 [00:00<00:00, 108283.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@300 0.46445497630331756 ; se:  0.03441598128848769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from data_utils import get_reddit_data_with_heldout\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import sem\n",
    "\n",
    "reddit_knnlm_recommender = KNNLMForTuningHyperParams(\n",
    "    data_path= 'datasets/inspired/inspired_train.csv',\n",
    "    model_name_or_path = 'models/inspired'\n",
    ")\n",
    "\n",
    "k=20\n",
    "n_neighbors = [15, 30, 60, 90, 120, 150, 180]\n",
    "recall = []\n",
    "for num_posts_to_consider in n_neighbors:\n",
    "    hits_at_k = []\n",
    "    cache = dict()\n",
    "    for idx in tqdm(range(len(reddit_knnlm_recommender.validation_dataset['context'])), total=len(reddit_knnlm_recommender.validation_dataset['context'])):\n",
    "        query = reddit_knnlm_recommender.validation_dataset['context'][idx]\n",
    "        target = reddit_knnlm_recommender.movie_vocab[reddit_knnlm_recommender.validation_dataset['label'][idx]]\n",
    "        if query not in cache:\n",
    "            movie_counts = reddit_knnlm_recommender.count_based_probability(\n",
    "                query, \n",
    "                num_posts_to_consider=num_posts_to_consider, \n",
    "                return_logits=True\n",
    "            )\n",
    "            recommended_movies = np.array(reddit_knnlm_recommender.movie_vocab)[np.argsort(-movie_counts)]\n",
    "            recommended_movies = [e for e in recommended_movies if e in inspired_eligible_entities]\n",
    "        else:\n",
    "            recommended_movies = cache[query]\n",
    "        \n",
    "        hits_at_k.append(int(target in recommended_movies[:k]))\n",
    "    recall.append(np.mean(hits_at_k))\n",
    "\n",
    "\n",
    "best_n_neighbors = n_neighbors[np.argmax(recall)]\n",
    "print('the recommended number of neighbors to use is ', best_n_neighbors)\n",
    "\n",
    "testset = pd.read_csv('datasets/inspired/inspired_test.csv')\n",
    "test_inputs = testset['test_inputs']\n",
    "test_groundtruths = testset['test_outputs']\n",
    "\n",
    "reddit_knnlm_recommender = KNNLMRecommender(\n",
    "    data_path= 'datasets/inspired/inspired_train.csv',\n",
    "    model_name_or_path = 'models/inspired'\n",
    ")\n",
    "\n",
    "K = [1,5, 10, 20,50,100,300]\n",
    "\n",
    "print('-------------------- Retrieval -------------------------------')\n",
    "\n",
    "cache = dict()\n",
    "for k in K:\n",
    "    hits_at_k = []\n",
    "    for idx in tqdm(range(len(test_inputs)), total = len(test_inputs) ):\n",
    "        query = test_inputs[idx]\n",
    "        target = test_groundtruths[idx]\n",
    "        \n",
    "        if query not in cache:\n",
    "            movie_counts = reddit_knnlm_recommender.count_based_probability(\n",
    "                query, \n",
    "                num_posts_to_consider=best_n_neighbors, \n",
    "                return_logits=True\n",
    "            )\n",
    "            recommended_movies = np.array(reddit_knnlm_recommender.movie_vocab)[np.argsort(-movie_counts)]\n",
    "            recommended_movies = [e for e in recommended_movies if e in inspired_eligible_entities]\n",
    "            cache[query] = recommended_movies\n",
    "        else:\n",
    "            recommended_movies = cache[query]\n",
    "        hits_at_k.append(int(target in recommended_movies[:k]))\n",
    "    print('r@'+str(k), np.mean(hits_at_k),'; se: ', sem(hits_at_k))\n",
    "\n",
    "print('-------------------- Recommend -------------------------------')\n",
    "\n",
    "cache = dict()\n",
    "for k in K:\n",
    "    hits_at_k = []\n",
    "    for idx in tqdm(range(len(test_inputs)), total = len(test_inputs) ):\n",
    "        query = test_inputs[idx]\n",
    "        target = test_groundtruths[idx]\n",
    "        \n",
    "        if query not in cache:\n",
    "            movie_probas = reddit_knnlm_recommender.predictor_probability(\n",
    "                query, \n",
    "                return_logits=False\n",
    "            )\n",
    "            recommended_movies = np.array(reddit_knnlm_recommender.movie_vocab)[np.argsort(-movie_probas)]\n",
    "            recommended_movies = [e for e in recommended_movies if e in inspired_eligible_entities]\n",
    "            cache[query] = recommended_movies\n",
    "        else:\n",
    "            recommended_movies = cache[query]\n",
    "        hits_at_k.append(int(target in recommended_movies[:k]))\n",
    "    print('r@'+str(k), np.mean(hits_at_k),'; se: ', sem(hits_at_k))\n",
    "\n",
    "\n",
    "print('-------------------- R+R (rerank) -------------------------------')\n",
    "\n",
    "cache = dict()\n",
    "for k in K:\n",
    "    hits_at_k = []\n",
    "    for idx in tqdm(range(len(test_inputs)), total = len(test_inputs) ):\n",
    "        query = test_inputs[idx]\n",
    "        target = test_groundtruths[idx]\n",
    "        \n",
    "        if query not in cache:\n",
    "            movie_counts = reddit_knnlm_recommender.count_based_probability(\n",
    "                query, \n",
    "                num_posts_to_consider=best_n_neighbors, \n",
    "                return_logits=True\n",
    "            )\n",
    "            movie_probas = reddit_knnlm_recommender.predictor_probability(\n",
    "                query, \n",
    "                return_logits=False\n",
    "            )\n",
    "            movie_scores = movie_counts + movie_probas\n",
    "            recommended_movies = np.array(reddit_knnlm_recommender.movie_vocab)[np.argsort(-movie_scores)]\n",
    "            recommended_movies = [e for e in recommended_movies if e in inspired_eligible_entities]\n",
    "            cache[query] = recommended_movies\n",
    "        else:\n",
    "            recommended_movies = cache[query]\n",
    "        hits_at_k.append(int(target in recommended_movies[:k]))\n",
    "    print('r@'+str(k), np.mean(hits_at_k),'; se: ', sem(hits_at_k))\n",
    "\n",
    "print('-------------------- R+R (rerank) with small gamma (cleaner solution for paper) -------------------------------')\n",
    "\n",
    "cache = dict()\n",
    "for k in K:\n",
    "    hits_at_k = []\n",
    "    for idx in tqdm(range(len(test_inputs)), total = len(test_inputs) ):\n",
    "        query = test_inputs[idx]\n",
    "        target = test_groundtruths[idx]\n",
    "        \n",
    "        if query not in cache:\n",
    "            movie_counts = reddit_knnlm_recommender.count_based_probability(\n",
    "                query, \n",
    "                num_posts_to_consider=best_n_neighbors, \n",
    "                return_logits=False\n",
    "            )\n",
    "            movie_probas = reddit_knnlm_recommender.predictor_probability(\n",
    "                query, \n",
    "                return_logits=False\n",
    "            )\n",
    "            movie_scores = movie_counts*(1-(1e-10)) + movie_probas*1e-10\n",
    "            recommended_movies = np.array(reddit_knnlm_recommender.movie_vocab)[np.argsort(-movie_scores)]\n",
    "            recommended_movies = [e for e in recommended_movies if e in inspired_eligible_entities]\n",
    "            cache[query] = recommended_movies\n",
    "        else:\n",
    "            recommended_movies = cache[query]\n",
    "        hits_at_k.append(int(target in recommended_movies[:k]))\n",
    "    print('r@'+str(k), np.mean(hits_at_k),'; se: ', sem(hits_at_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f12b635-5ce5-44d6-9db2-7091128e0b72",
   "metadata": {},
   "source": [
    "## Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8289bf8f-b892-4964-87ac-ee02e2be30dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interaction preservance rate at 20000 items\n",
      "0.9965472793625889\n",
      "num items  20000\n",
      "flattening posts into training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 39928/39928 [00:00<00:00, 390030.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building datastore embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "621it [01:02,  9.96it/s]                                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- R+R (rerank) with small gamma (cleaner solution for paper) -------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 19438/19438 [02:09<00:00, 150.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@1 0.012604177384504579 ; se:  0.0008001810805301169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 19438/19438 [00:00<00:00, 122067.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@5 0.05957403025002572 ; se:  0.0016977595429411435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 19438/19438 [00:00<00:00, 122939.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@10 0.1020681140034983 ; se:  0.0021714614543734746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 19438/19438 [00:00<00:00, 122901.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@20 0.1558802345920362 ; se:  0.002601854118789058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 19438/19438 [00:00<00:00, 120070.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@50 0.2576396748636691 ; se:  0.003136888849981628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 19438/19438 [00:00<00:00, 118067.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@100 0.34556024282333575 ; se:  0.003411003031134365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 19438/19438 [00:00<00:00, 108624.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@300 0.47715814384195904 ; se:  0.0035826280908021206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from data_utils import get_reddit_data_with_heldout\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import sem\n",
    "\n",
    "# reddit_knnlm_recommender = KNNLMForTuningHyperParams(\n",
    "#     data_path= 'datasets/reddit/reddit_large_train.csv',\n",
    "#     model_name_or_path = 'models/reddit'\n",
    "# )\n",
    "\n",
    "# k=20\n",
    "# n_neighbors = [15, 30, 60, 90, 120, 150, 180]\n",
    "# recall = []\n",
    "# for num_posts_to_consider in n_neighbors:\n",
    "#     hits_at_k = []\n",
    "#     cache = dict()\n",
    "#     for idx in tqdm(range(len(reddit_knnlm_recommender.validation_dataset['context'])), total=len(reddit_knnlm_recommender.validation_dataset['context'])):\n",
    "#         query = reddit_knnlm_recommender.validation_dataset['context'][idx]\n",
    "#         target = reddit_knnlm_recommender.movie_vocab[reddit_knnlm_recommender.validation_dataset['label'][idx]]\n",
    "#         if query not in cache:\n",
    "#             movie_counts = reddit_knnlm_recommender.count_based_probability(\n",
    "#                 query, \n",
    "#                 num_posts_to_consider=num_posts_to_consider, \n",
    "#                 return_logits=True\n",
    "#             )\n",
    "#             recommended_movies = np.array(reddit_knnlm_recommender.movie_vocab)[np.argsort(-movie_counts)]\n",
    "#         else:\n",
    "#             recommended_movies = cache[query]\n",
    "        \n",
    "#         hits_at_k.append(int(target in recommended_movies[:k]))\n",
    "#     recall.append(np.mean(hits_at_k))\n",
    "\n",
    "\n",
    "# best_n_neighbors = n_neighbors[np.argmax(recall)]\n",
    "# print('the recommended number of neighbors to use is ', best_n_neighbors)\n",
    "\n",
    "best_n_neighbors = 30 # uncomment above to tune n_neighbors\n",
    "\n",
    "testset = pd.read_csv('datasets/reddit/reddit_test.csv')\n",
    "test_inputs = testset['test_inputs']\n",
    "test_groundtruths = testset['test_outputs']\n",
    "\n",
    "reddit_knnlm_recommender = KNNLMRecommender(\n",
    "    data_path= 'datasets/reddit/reddit_large_train.csv',\n",
    "    model_name_or_path = 'models/reddit'\n",
    ")\n",
    "\n",
    "K = [1,5, 10, 20,50,100,300]\n",
    "\n",
    "# print('-------------------- Retrieval -------------------------------')\n",
    "\n",
    "# cache = dict()\n",
    "# for k in K:\n",
    "#     hits_at_k = []\n",
    "#     for idx in tqdm(range(len(test_inputs)), total = len(test_inputs) ):\n",
    "#         query = test_inputs[idx]\n",
    "#         target = test_groundtruths[idx]\n",
    "        \n",
    "#         if query not in cache:\n",
    "#             movie_counts = reddit_knnlm_recommender.count_based_probability(\n",
    "#                 query, \n",
    "#                 num_posts_to_consider=best_n_neighbors, \n",
    "#                 return_logits=True\n",
    "#             )\n",
    "#             recommended_movies = np.array(reddit_knnlm_recommender.movie_vocab)[np.argsort(-movie_counts)]\n",
    "#             cache[query] = recommended_movies\n",
    "#         else:\n",
    "#             recommended_movies = cache[query]\n",
    "#         hits_at_k.append(int(target in recommended_movies[:k]))\n",
    "#     print('r@'+str(k), np.mean(hits_at_k),'; se: ', sem(hits_at_k))\n",
    "\n",
    "# print('-------------------- Recommend -------------------------------')\n",
    "\n",
    "# cache = dict()\n",
    "# for k in K:\n",
    "#     hits_at_k = []\n",
    "#     for idx in tqdm(range(len(test_inputs)), total = len(test_inputs) ):\n",
    "#         query = test_inputs[idx]\n",
    "#         target = test_groundtruths[idx]\n",
    "        \n",
    "#         if query not in cache:\n",
    "#             movie_probas = reddit_knnlm_recommender.predictor_probability(\n",
    "#                 query, \n",
    "#                 return_logits=False\n",
    "#             )\n",
    "#             recommended_movies = np.array(reddit_knnlm_recommender.movie_vocab)[np.argsort(-movie_probas)]\n",
    "#             cache[query] = recommended_movies\n",
    "#         else:\n",
    "#             recommended_movies = cache[query]\n",
    "#         hits_at_k.append(int(target in recommended_movies[:k]))\n",
    "#     print('r@'+str(k), np.mean(hits_at_k),'; se: ', sem(hits_at_k))\n",
    "\n",
    "\n",
    "# print('-------------------- R+R (rerank) -------------------------------')\n",
    "\n",
    "# cache = dict()\n",
    "# for k in K:\n",
    "#     hits_at_k = []\n",
    "#     for idx in tqdm(range(len(test_inputs)), total = len(test_inputs) ):\n",
    "#         query = test_inputs[idx]\n",
    "#         target = test_groundtruths[idx]\n",
    "        \n",
    "#         if query not in cache:\n",
    "#             movie_counts = reddit_knnlm_recommender.count_based_probability(\n",
    "#                 query, \n",
    "#                 num_posts_to_consider=best_n_neighbors, \n",
    "#                 return_logits=True\n",
    "#             )\n",
    "#             movie_probas = reddit_knnlm_recommender.predictor_probability(\n",
    "#                 query, \n",
    "#                 return_logits=False\n",
    "#             )\n",
    "#             movie_scores = movie_counts + movie_probas\n",
    "#             recommended_movies = np.array(reddit_knnlm_recommender.movie_vocab)[np.argsort(-movie_scores)]\n",
    "#             cache[query] = recommended_movies\n",
    "#         else:\n",
    "#             recommended_movies = cache[query]\n",
    "#         hits_at_k.append(int(target in recommended_movies[:k]))\n",
    "#     print('r@'+str(k), np.mean(hits_at_k),'; se: ', sem(hits_at_k))\n",
    "\n",
    "print('-------------------- R+R (rerank) with small gamma (cleaner solution for paper) -------------------------------')\n",
    "\n",
    "cache = dict()\n",
    "for k in K:\n",
    "    hits_at_k = []\n",
    "    for idx in tqdm(range(len(test_inputs)), total = len(test_inputs) ):\n",
    "        query = test_inputs[idx]\n",
    "        target = test_groundtruths[idx]\n",
    "        \n",
    "        if query not in cache:\n",
    "            movie_counts = reddit_knnlm_recommender.count_based_probability(\n",
    "                query, \n",
    "                num_posts_to_consider=best_n_neighbors, \n",
    "                return_logits=False\n",
    "            )\n",
    "            movie_probas = reddit_knnlm_recommender.predictor_probability(\n",
    "                query, \n",
    "                return_logits=False\n",
    "            )\n",
    "            movie_scores = movie_counts*(1-(1e-10)) + movie_probas*1e-10\n",
    "            recommended_movies = np.array(reddit_knnlm_recommender.movie_vocab)[np.argsort(-movie_scores)]\n",
    "            cache[query] = recommended_movies\n",
    "        else:\n",
    "            recommended_movies = cache[query]\n",
    "        hits_at_k.append(int(target in recommended_movies[:k]))\n",
    "    print('r@'+str(k), np.mean(hits_at_k),'; se: ', sem(hits_at_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a88772-c0f0-4b09-be87-609aeadcd0dc",
   "metadata": {},
   "source": [
    "## Redial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c9cae71-cfce-4a8d-a343-d3a138d77569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interaction preservance rate at 20000 items\n",
      "1.0\n",
      "num items  5140\n",
      "flattening posts into training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 8929/8929 [00:00<00:00, 1403918.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building datastore embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "136it [00:10, 13.37it/s]                                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Retrieval -------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:58<00:00, 73.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@1 0.014225746268656716 ; se:  0.0018086291367414823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:00<00:00, 186717.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@5 0.07882462686567164 ; se:  0.004115526282453156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:00<00:00, 203316.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@10 0.13036380597014927 ; se:  0.005142455913816717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:00<00:00, 197717.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@20 0.19496268656716417 ; se:  0.006050719409148609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:00<00:00, 169243.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@50 0.2716884328358209 ; se:  0.00679387319005625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:00<00:00, 143212.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@100 0.32532649253731344 ; se:  0.007155332211070106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:00<00:00, 98362.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@300 0.3558768656716418 ; se:  0.007312360356578281\n",
      "-------------------- Recommend -------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:26<00:00, 160.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@1 0.01632462686567164 ; se:  0.001935400234206339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:00<00:00, 186423.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@5 0.06529850746268656 ; se:  0.003773213274540625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:00<00:00, 213031.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@10 0.11287313432835822 ; se:  0.004832940144924738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:00<00:00, 199076.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@20 0.17444029850746268 ; se:  0.005795900370660485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:00<00:00, 168794.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@50 0.27705223880597013 ; se:  0.006835299502280828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:00<00:00, 141657.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@100 0.3631063432835821 ; se:  0.007344692838232835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:00<00:00, 101766.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@300 0.5170242537313433 ; se:  0.0076320529990241736\n",
      "-------------------- R+R (rerank) -------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [01:11<00:00, 60.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@1 0.013292910447761194 ; se:  0.0017491514807566862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:00<00:00, 188326.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@5 0.078125 ; se:  0.0040987770162482185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:00<00:00, 211073.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@10 0.12896455223880596 ; se:  0.005118896520469577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:00<00:00, 198140.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@20 0.19776119402985073 ; se:  0.006083389571891694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:00<00:00, 172100.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@50 0.2896455223880597 ; se:  0.006927782253933873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:00<00:00, 142139.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@100 0.37569962686567165 ; se:  0.007396741432586565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:00<00:00, 101724.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@300 0.5298507462686567 ; se:  0.007622859390812175\n",
      "-------------------- R+R (rerank) with small gamma (cleaner solution for paper) -------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [01:11<00:00, 59.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@1 0.013292910447761194 ; se:  0.0017491514807566862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:00<00:00, 191014.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@5 0.07695895522388059 ; se:  0.004070646097800788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:00<00:00, 209246.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@10 0.12569962686567165 ; se:  0.005063147557043472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:00<00:00, 196821.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@20 0.1970615671641791 ; se:  0.006075266692605248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:00<00:00, 166043.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@50 0.2908115671641791 ; se:  0.006936013318055069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:00<00:00, 139619.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@100 0.3763992537313433 ; se:  0.0073994756987399045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4288/4288 [00:00<00:00, 102445.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r@300 0.5293843283582089 ; se:  0.0076232820990675245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from data_utils import get_reddit_data_with_heldout\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import sem\n",
    "\n",
    "# reddit_knnlm_recommender = KNNLMForTuningHyperParams(\n",
    "#     data_path= 'datasets/redial/redial_train.csv',\n",
    "#     model_name_or_path = 'models/redial'\n",
    "# )\n",
    "\n",
    "# k=20\n",
    "# n_neighbors = [15, 30, 60, 90, 120, 150, 180]\n",
    "# recall = []\n",
    "# for num_posts_to_consider in n_neighbors:\n",
    "#     hits_at_k = []\n",
    "#     cache = dict()\n",
    "#     for idx in tqdm(range(len(reddit_knnlm_recommender.validation_dataset['context'])), total=len(reddit_knnlm_recommender.validation_dataset['context'])):\n",
    "#         query = reddit_knnlm_recommender.validation_dataset['context'][idx]\n",
    "#         target = reddit_knnlm_recommender.movie_vocab[reddit_knnlm_recommender.validation_dataset['label'][idx]]\n",
    "#         if query not in cache:\n",
    "#             movie_counts = reddit_knnlm_recommender.count_based_probability(\n",
    "#                 query, \n",
    "#                 num_posts_to_consider=num_posts_to_consider, \n",
    "#                 return_logits=True\n",
    "#             )\n",
    "#             recommended_movies = np.array(reddit_knnlm_recommender.movie_vocab)[np.argsort(-movie_counts)]\n",
    "#             recommended_movies = [e for e in recommended_movies if e in redial_eligible_entities]\n",
    "#         else:\n",
    "#             recommended_movies = cache[query]\n",
    "        \n",
    "#         hits_at_k.append(int(target in recommended_movies[:k]))\n",
    "#     recall.append(np.mean(hits_at_k))\n",
    "\n",
    "\n",
    "# best_n_neighbors = n_neighbors[np.argmax(recall)]\n",
    "# print('the recommended number of neighbors to use is ', best_n_neighbors)\n",
    "\n",
    "best_n_neighbors = 60\n",
    "\n",
    "testset = pd.read_csv('datasets/redial/redial_test.csv')\n",
    "test_inputs = testset['test_inputs']\n",
    "test_groundtruths = testset['test_outputs']\n",
    "\n",
    "reddit_knnlm_recommender = KNNLMRecommender(\n",
    "    data_path= 'datasets/redial/redial_train.csv',\n",
    "    model_name_or_path = 'models/redial'\n",
    ")\n",
    "\n",
    "K = [1,5, 10, 20,50,100,300]\n",
    "\n",
    "print('-------------------- Retrieval -------------------------------')\n",
    "\n",
    "cache = dict()\n",
    "for k in K:\n",
    "    hits_at_k = []\n",
    "    for idx in tqdm(range(len(test_inputs)), total = len(test_inputs) ):\n",
    "        query = test_inputs[idx]\n",
    "        target = test_groundtruths[idx]\n",
    "        \n",
    "        if query not in cache:\n",
    "            movie_counts = reddit_knnlm_recommender.count_based_probability(\n",
    "                query, \n",
    "                num_posts_to_consider=best_n_neighbors, \n",
    "                return_logits=True\n",
    "            )\n",
    "            recommended_movies = np.array(reddit_knnlm_recommender.movie_vocab)[np.argsort(-movie_counts)]\n",
    "            recommended_movies = [e for e in recommended_movies if e in redial_eligible_entities]\n",
    "            cache[query] = recommended_movies\n",
    "        else:\n",
    "            recommended_movies = cache[query]\n",
    "        hits_at_k.append(int(target in recommended_movies[:k]))\n",
    "    print('r@'+str(k), np.mean(hits_at_k),'; se: ', sem(hits_at_k))\n",
    "\n",
    "print('-------------------- Recommend -------------------------------')\n",
    "\n",
    "cache = dict()\n",
    "for k in K:\n",
    "    hits_at_k = []\n",
    "    for idx in tqdm(range(len(test_inputs)), total = len(test_inputs) ):\n",
    "        query = test_inputs[idx]\n",
    "        target = test_groundtruths[idx]\n",
    "        \n",
    "        if query not in cache:\n",
    "            movie_probas = reddit_knnlm_recommender.predictor_probability(\n",
    "                query, \n",
    "                return_logits=False\n",
    "            )\n",
    "            recommended_movies = np.array(reddit_knnlm_recommender.movie_vocab)[np.argsort(-movie_probas)]\n",
    "            recommended_movies = [e for e in recommended_movies if e in redial_eligible_entities]\n",
    "            cache[query] = recommended_movies\n",
    "        else:\n",
    "            recommended_movies = cache[query]\n",
    "        hits_at_k.append(int(target in recommended_movies[:k]))\n",
    "    print('r@'+str(k), np.mean(hits_at_k),'; se: ', sem(hits_at_k))\n",
    "\n",
    "\n",
    "print('-------------------- R+R (rerank) -------------------------------')\n",
    "\n",
    "cache = dict()\n",
    "for k in K:\n",
    "    hits_at_k = []\n",
    "    for idx in tqdm(range(len(test_inputs)), total = len(test_inputs) ):\n",
    "        query = test_inputs[idx]\n",
    "        target = test_groundtruths[idx]\n",
    "        \n",
    "        if query not in cache:\n",
    "            movie_counts = reddit_knnlm_recommender.count_based_probability(\n",
    "                query, \n",
    "                num_posts_to_consider=best_n_neighbors, \n",
    "                return_logits=True\n",
    "            )\n",
    "            movie_probas = reddit_knnlm_recommender.predictor_probability(\n",
    "                query, \n",
    "                return_logits=False\n",
    "            )\n",
    "            movie_scores = movie_counts + movie_probas\n",
    "            recommended_movies = np.array(reddit_knnlm_recommender.movie_vocab)[np.argsort(-movie_scores)]\n",
    "            recommended_movies = [e for e in recommended_movies if e in redial_eligible_entities]\n",
    "            cache[query] = recommended_movies\n",
    "        else:\n",
    "            recommended_movies = cache[query]\n",
    "        hits_at_k.append(int(target in recommended_movies[:k]))\n",
    "    print('r@'+str(k), np.mean(hits_at_k),'; se: ', sem(hits_at_k))\n",
    "\n",
    "\n",
    "print('-------------------- R+R (rerank) with small gamma (cleaner solution for paper) -------------------------------')\n",
    "\n",
    "cache = dict()\n",
    "for k in K:\n",
    "    hits_at_k = []\n",
    "    for idx in tqdm(range(len(test_inputs)), total = len(test_inputs) ):\n",
    "        query = test_inputs[idx]\n",
    "        target = test_groundtruths[idx]\n",
    "        \n",
    "        if query not in cache:\n",
    "            movie_counts = reddit_knnlm_recommender.count_based_probability(\n",
    "                query, \n",
    "                num_posts_to_consider=best_n_neighbors, \n",
    "                return_logits=False\n",
    "            )\n",
    "            movie_probas = reddit_knnlm_recommender.predictor_probability(\n",
    "                query, \n",
    "                return_logits=False\n",
    "            )\n",
    "            movie_scores = movie_counts*(1-(1e-10)) + movie_probas*1e-10\n",
    "            recommended_movies = np.array(reddit_knnlm_recommender.movie_vocab)[np.argsort(-movie_scores)]\n",
    "            recommended_movies = [e for e in recommended_movies if e in redial_eligible_entities]\n",
    "            cache[query] = recommended_movies\n",
    "        else:\n",
    "            recommended_movies = cache[query]\n",
    "        hits_at_k.append(int(target in recommended_movies[:k]))\n",
    "    print('r@'+str(k), np.mean(hits_at_k),'; se: ', sem(hits_at_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a27da4-74b7-402a-9b0b-35c9978a6c97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baize",
   "language": "python",
   "name": "baize"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
